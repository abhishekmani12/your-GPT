#CHROMA EMBEDDINGS:

EMB_CHUNK_SIZE = 256
EMB_CHUNK_OVERLAP = 32

#QDRANT EMBEDDINGS:
QEMB_CHUNK_SIZE = 200
QEMB_CHUNK_OVERLAP = 32

#LLAMACPP Arguments:

SOURCE_CHUNKS = 3
TOP_P = 0.4
TOP_K = 40
TEMPERATURE = 0.4
REPEAT_PENALTY = 1.18
CTX = 2048
MAX_TOKENS = 4096
BATCH_SIZE = 128

#GPT4ALL Arguments:

GPT4ALL_BATCH_SIZE = 512

#GAI Arguments:

PALM_MODEL = 'models/chat-bison-001'
PALM_TEMPERATURE = 0.2
PALM_CANDIDATE_COUNT = 1
PALM_TOP_K  = 40
PALM_TOP_P = 0.95

GEM_MODEL = 'gemini-pro'
GEM_TEMPERATURE = 0.9
GEM_MAX_TOKENS = 2048
GEM_TOP_K  = 40
GEM_TOP_P = 0.95

#WIKI Arguments:

USER_AGENT = 'Local-Mini-GPT'

